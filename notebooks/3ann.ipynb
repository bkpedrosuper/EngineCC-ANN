{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build an artificial neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from config import MAIN_PALETTE\n",
    "\n",
    "sns.set_theme(context='notebook', style='whitegrid', palette='bright', font='sans-serif', \n",
    "                  font_scale=1, color_codes=True, rc=None)\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_201</th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_792</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682921</td>\n",
       "      <td>0.682195</td>\n",
       "      <td>0.681640</td>\n",
       "      <td>0.681171</td>\n",
       "      <td>0.680729</td>\n",
       "      <td>0.680484</td>\n",
       "      <td>0.680928</td>\n",
       "      <td>0.682209</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.684659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406982</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.402194</td>\n",
       "      <td>0.448523</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.274347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.679731</td>\n",
       "      <td>0.678763</td>\n",
       "      <td>0.678157</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.679554</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.681912</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>0.682467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273656</td>\n",
       "      <td>0.261276</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.356592</td>\n",
       "      <td>0.389055</td>\n",
       "      <td>0.355631</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692921</td>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.690065</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.687545</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.685337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.342483</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.278585</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684316</td>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.684634</td>\n",
       "      <td>0.684632</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.683996</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.682420</td>\n",
       "      <td>0.681621</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265779</td>\n",
       "      <td>0.276489</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.568265</td>\n",
       "      <td>0.505633</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.407461</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671236</td>\n",
       "      <td>0.670445</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.667360</td>\n",
       "      <td>0.668124</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415693</td>\n",
       "      <td>0.381110</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.460601</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.281171</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.655160</td>\n",
       "      <td>0.654703</td>\n",
       "      <td>0.654392</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>0.654086</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.655265</td>\n",
       "      <td>0.656777</td>\n",
       "      <td>0.658473</td>\n",
       "      <td>0.659922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085348</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.688720</td>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.693366</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.696822</td>\n",
       "      <td>0.698040</td>\n",
       "      <td>0.699004</td>\n",
       "      <td>0.699738</td>\n",
       "      <td>0.700179</td>\n",
       "      <td>0.700178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243962</td>\n",
       "      <td>0.250543</td>\n",
       "      <td>0.279990</td>\n",
       "      <td>0.333953</td>\n",
       "      <td>0.393716</td>\n",
       "      <td>0.419058</td>\n",
       "      <td>0.355549</td>\n",
       "      <td>0.327488</td>\n",
       "      <td>0.312347</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.717838</td>\n",
       "      <td>0.716938</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.714911</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.712956</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>0.710904</td>\n",
       "      <td>0.709947</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>0.262787</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.268023</td>\n",
       "      <td>0.270470</td>\n",
       "      <td>0.250558</td>\n",
       "      <td>0.187487</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>0.156544</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.671228</td>\n",
       "      <td>0.671696</td>\n",
       "      <td>0.671971</td>\n",
       "      <td>0.672160</td>\n",
       "      <td>0.672397</td>\n",
       "      <td>0.672658</td>\n",
       "      <td>0.672790</td>\n",
       "      <td>0.672632</td>\n",
       "      <td>0.672193</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.139477</td>\n",
       "      <td>0.127960</td>\n",
       "      <td>0.122262</td>\n",
       "      <td>0.118875</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.722550</td>\n",
       "      <td>0.721452</td>\n",
       "      <td>0.720337</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.716881</td>\n",
       "      <td>0.715988</td>\n",
       "      <td>0.715285</td>\n",
       "      <td>0.714784</td>\n",
       "      <td>0.714635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158205</td>\n",
       "      <td>0.208111</td>\n",
       "      <td>0.335710</td>\n",
       "      <td>0.476410</td>\n",
       "      <td>0.531009</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.308952</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.165598</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_201  current_202  current_203  current_204  current_205  \\\n",
       "0       0.682921     0.682195     0.681640     0.681171     0.680729   \n",
       "1       0.679731     0.678763     0.678157     0.678000     0.678474   \n",
       "2       0.692921     0.692640     0.692082     0.691222     0.690065   \n",
       "3       0.684316     0.684550     0.684634     0.684632     0.684461   \n",
       "4       0.671236     0.670445     0.669661     0.668889     0.668196   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "474     0.655160     0.654703     0.654392     0.654158     0.654086   \n",
       "475     0.688720     0.691066     0.693366     0.695296     0.696822   \n",
       "476     0.717838     0.716938     0.715976     0.714911     0.713888   \n",
       "477     0.671228     0.671696     0.671971     0.672160     0.672397   \n",
       "478     0.722550     0.721452     0.720337     0.719178     0.717978   \n",
       "\n",
       "     current_206  current_207  current_208  current_209  current_210  ...  \\\n",
       "0       0.680484     0.680928     0.682209     0.683694     0.684659  ...   \n",
       "1       0.679554     0.680862     0.681912     0.682436     0.682467  ...   \n",
       "2       0.688761     0.687545     0.686558     0.685798     0.685337  ...   \n",
       "3       0.683996     0.683266     0.682420     0.681621     0.680988  ...   \n",
       "4       0.667722     0.667399     0.667360     0.668124     0.669890  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "474     0.654370     0.655265     0.656777     0.658473     0.659922  ...   \n",
       "475     0.698040     0.699004     0.699738     0.700179     0.700178  ...   \n",
       "476     0.712956     0.711976     0.710904     0.709947     0.709364  ...   \n",
       "477     0.672658     0.672790     0.672632     0.672193     0.671623  ...   \n",
       "478     0.716881     0.715988     0.715285     0.714784     0.714635  ...   \n",
       "\n",
       "     volt_792  volt_793  volt_794  volt_795  volt_796  volt_797  volt_798  \\\n",
       "0    0.406982  0.367278  0.364093  0.402194  0.448523  0.450469  0.356803   \n",
       "1    0.273656  0.261276  0.271836  0.308600  0.356592  0.389055  0.355631   \n",
       "2    0.317711  0.297014  0.288137  0.302547  0.330311  0.342483  0.289035   \n",
       "3    0.265779  0.276489  0.310451  0.382017  0.485955  0.568265  0.505633   \n",
       "4    0.415693  0.381110  0.376050  0.407861  0.460601  0.481486  0.389891   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "474  0.085348  0.067686  0.062863  0.078773  0.102914  0.117693  0.097859   \n",
       "475  0.243962  0.250543  0.279990  0.333953  0.393716  0.419058  0.355549   \n",
       "476  0.269906  0.262787  0.261464  0.268023  0.270470  0.250558  0.187487   \n",
       "477  0.143442  0.139477  0.127960  0.122262  0.118875  0.110584  0.083422   \n",
       "478  0.158205  0.208111  0.335710  0.476410  0.531009  0.470152  0.308952   \n",
       "\n",
       "     volt_799  volt_800  state  \n",
       "0    0.302953  0.274347      0  \n",
       "1    0.369090  0.410403      0  \n",
       "2    0.271342  0.278585      0  \n",
       "3    0.450684  0.407461      0  \n",
       "4    0.325563  0.281171      0  \n",
       "..        ...       ...    ...  \n",
       "474  0.085690  0.079646      2  \n",
       "475  0.327488  0.312347      2  \n",
       "476  0.160850  0.156544      2  \n",
       "477  0.073359  0.071337      2  \n",
       "478  0.213821  0.165598      2  \n",
       "\n",
       "[479 rows x 1201 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/dataset_cleaned.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_201</th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_791</th>\n",
       "      <th>volt_792</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682921</td>\n",
       "      <td>0.682195</td>\n",
       "      <td>0.681640</td>\n",
       "      <td>0.681171</td>\n",
       "      <td>0.680729</td>\n",
       "      <td>0.680484</td>\n",
       "      <td>0.680928</td>\n",
       "      <td>0.682209</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.684659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475171</td>\n",
       "      <td>0.406982</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.402194</td>\n",
       "      <td>0.448523</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.274347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.679731</td>\n",
       "      <td>0.678763</td>\n",
       "      <td>0.678157</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.679554</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.681912</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>0.682467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313650</td>\n",
       "      <td>0.273656</td>\n",
       "      <td>0.261276</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.356592</td>\n",
       "      <td>0.389055</td>\n",
       "      <td>0.355631</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.410403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692921</td>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.690065</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.687545</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.685337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346655</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.342483</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.278585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684316</td>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.684634</td>\n",
       "      <td>0.684632</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.683996</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.682420</td>\n",
       "      <td>0.681621</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277664</td>\n",
       "      <td>0.265779</td>\n",
       "      <td>0.276489</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.568265</td>\n",
       "      <td>0.505633</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.407461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671236</td>\n",
       "      <td>0.670445</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.667360</td>\n",
       "      <td>0.668124</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.415693</td>\n",
       "      <td>0.381110</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.460601</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.281171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.655160</td>\n",
       "      <td>0.654703</td>\n",
       "      <td>0.654392</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>0.654086</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.655265</td>\n",
       "      <td>0.656777</td>\n",
       "      <td>0.658473</td>\n",
       "      <td>0.659922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118411</td>\n",
       "      <td>0.085348</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>0.079646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.688720</td>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.693366</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.696822</td>\n",
       "      <td>0.698040</td>\n",
       "      <td>0.699004</td>\n",
       "      <td>0.699738</td>\n",
       "      <td>0.700179</td>\n",
       "      <td>0.700178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267515</td>\n",
       "      <td>0.243962</td>\n",
       "      <td>0.250543</td>\n",
       "      <td>0.279990</td>\n",
       "      <td>0.333953</td>\n",
       "      <td>0.393716</td>\n",
       "      <td>0.419058</td>\n",
       "      <td>0.355549</td>\n",
       "      <td>0.327488</td>\n",
       "      <td>0.312347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.717838</td>\n",
       "      <td>0.716938</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.714911</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.712956</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>0.710904</td>\n",
       "      <td>0.709947</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300854</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>0.262787</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.268023</td>\n",
       "      <td>0.270470</td>\n",
       "      <td>0.250558</td>\n",
       "      <td>0.187487</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>0.156544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.671228</td>\n",
       "      <td>0.671696</td>\n",
       "      <td>0.671971</td>\n",
       "      <td>0.672160</td>\n",
       "      <td>0.672397</td>\n",
       "      <td>0.672658</td>\n",
       "      <td>0.672790</td>\n",
       "      <td>0.672632</td>\n",
       "      <td>0.672193</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149968</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.139477</td>\n",
       "      <td>0.127960</td>\n",
       "      <td>0.122262</td>\n",
       "      <td>0.118875</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.071337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.722550</td>\n",
       "      <td>0.721452</td>\n",
       "      <td>0.720337</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.716881</td>\n",
       "      <td>0.715988</td>\n",
       "      <td>0.715285</td>\n",
       "      <td>0.714784</td>\n",
       "      <td>0.714635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169253</td>\n",
       "      <td>0.158205</td>\n",
       "      <td>0.208111</td>\n",
       "      <td>0.335710</td>\n",
       "      <td>0.476410</td>\n",
       "      <td>0.531009</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.308952</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.165598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows × 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_201  current_202  current_203  current_204  current_205  \\\n",
       "0       0.682921     0.682195     0.681640     0.681171     0.680729   \n",
       "1       0.679731     0.678763     0.678157     0.678000     0.678474   \n",
       "2       0.692921     0.692640     0.692082     0.691222     0.690065   \n",
       "3       0.684316     0.684550     0.684634     0.684632     0.684461   \n",
       "4       0.671236     0.670445     0.669661     0.668889     0.668196   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "474     0.655160     0.654703     0.654392     0.654158     0.654086   \n",
       "475     0.688720     0.691066     0.693366     0.695296     0.696822   \n",
       "476     0.717838     0.716938     0.715976     0.714911     0.713888   \n",
       "477     0.671228     0.671696     0.671971     0.672160     0.672397   \n",
       "478     0.722550     0.721452     0.720337     0.719178     0.717978   \n",
       "\n",
       "     current_206  current_207  current_208  current_209  current_210  ...  \\\n",
       "0       0.680484     0.680928     0.682209     0.683694     0.684659  ...   \n",
       "1       0.679554     0.680862     0.681912     0.682436     0.682467  ...   \n",
       "2       0.688761     0.687545     0.686558     0.685798     0.685337  ...   \n",
       "3       0.683996     0.683266     0.682420     0.681621     0.680988  ...   \n",
       "4       0.667722     0.667399     0.667360     0.668124     0.669890  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "474     0.654370     0.655265     0.656777     0.658473     0.659922  ...   \n",
       "475     0.698040     0.699004     0.699738     0.700179     0.700178  ...   \n",
       "476     0.712956     0.711976     0.710904     0.709947     0.709364  ...   \n",
       "477     0.672658     0.672790     0.672632     0.672193     0.671623  ...   \n",
       "478     0.716881     0.715988     0.715285     0.714784     0.714635  ...   \n",
       "\n",
       "     volt_791  volt_792  volt_793  volt_794  volt_795  volt_796  volt_797  \\\n",
       "0    0.475171  0.406982  0.367278  0.364093  0.402194  0.448523  0.450469   \n",
       "1    0.313650  0.273656  0.261276  0.271836  0.308600  0.356592  0.389055   \n",
       "2    0.346655  0.317711  0.297014  0.288137  0.302547  0.330311  0.342483   \n",
       "3    0.277664  0.265779  0.276489  0.310451  0.382017  0.485955  0.568265   \n",
       "4    0.474643  0.415693  0.381110  0.376050  0.407861  0.460601  0.481486   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "474  0.118411  0.085348  0.067686  0.062863  0.078773  0.102914  0.117693   \n",
       "475  0.267515  0.243962  0.250543  0.279990  0.333953  0.393716  0.419058   \n",
       "476  0.300854  0.269906  0.262787  0.261464  0.268023  0.270470  0.250558   \n",
       "477  0.149968  0.143442  0.139477  0.127960  0.122262  0.118875  0.110584   \n",
       "478  0.169253  0.158205  0.208111  0.335710  0.476410  0.531009  0.470152   \n",
       "\n",
       "     volt_798  volt_799  volt_800  \n",
       "0    0.356803  0.302953  0.274347  \n",
       "1    0.355631  0.369090  0.410403  \n",
       "2    0.289035  0.271342  0.278585  \n",
       "3    0.505633  0.450684  0.407461  \n",
       "4    0.389891  0.325563  0.281171  \n",
       "..        ...       ...       ...  \n",
       "474  0.097859  0.085690  0.079646  \n",
       "475  0.355549  0.327488  0.312347  \n",
       "476  0.187487  0.160850  0.156544  \n",
       "477  0.083422  0.073359  0.071337  \n",
       "478  0.308952  0.213821  0.165598  \n",
       "\n",
       "[479 rows x 1200 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best training and testing datasets, we're going to balance the amount of each state.\n",
    "We're going to use:\n",
    "\n",
    "Training:\n",
    "120 from state 0\n",
    "85 from state 1\n",
    "85 from state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0 shape: (288, 1201)\n",
      "State 1 shape: (97, 1201)\n",
      "State 2 shape: (94, 1201)\n"
     ]
    }
   ],
   "source": [
    "df_0 = data.query('state == 0')\n",
    "df_1 = data.query('state == 1')\n",
    "df_2 = data.query('state == 2')\n",
    "\n",
    "print('State 0 shape:', df_0.shape)\n",
    "print('State 1 shape:', df_1.shape)\n",
    "print('State 2 shape:', df_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (184, 1200)\n",
      "y_test shape (184,)\n",
      "==========\n",
      "X_train shape (295, 1200)\n",
      "y_train shape (295,)\n"
     ]
    }
   ],
   "source": [
    "state0_train_size = 125\n",
    "state1_train_size = 85\n",
    "state2_train_size = 85\n",
    "\n",
    "# state 0\n",
    "X_train_0 = df_0.iloc[:state0_train_size, :-1]\n",
    "X_test_0 = df_0.iloc[state0_train_size:, :-1]\n",
    "\n",
    "y_train_0 = df_0.iloc[:state0_train_size, -1]\n",
    "y_test_0 = df_0.iloc[state0_train_size:, -1]\n",
    "\n",
    "# state 1\n",
    "X_train_1 = df_1.iloc[:state1_train_size, :-1]\n",
    "X_test_1 = df_1.iloc[state1_train_size:, :-1]\n",
    "\n",
    "y_train_1 = df_1.iloc[:state1_train_size, -1]\n",
    "y_test_1 = df_1.iloc[state1_train_size:, -1]\n",
    "\n",
    "# state 2\n",
    "X_train_2 = df_2.iloc[:state2_train_size, :-1]\n",
    "X_test_2 = df_2.iloc[state2_train_size:, :-1]\n",
    "\n",
    "y_train_2 = df_2.iloc[:state2_train_size, -1]\n",
    "y_test_2 = df_2.iloc[state2_train_size:, -1]\n",
    "\n",
    "# Concatenate\n",
    "X_train = pd.concat([X_train_0, X_train_1, X_train_2])\n",
    "X_test = pd.concat([X_test_0, X_test_1, X_test_2])\n",
    "\n",
    "y_train = pd.concat([y_train_0, y_train_1, y_train_2])\n",
    "y_test = pd.concat([y_test_0, y_test_1, y_test_2])\n",
    "\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "print(\"==========\")\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_201</th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_791</th>\n",
       "      <th>volt_792</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682921</td>\n",
       "      <td>0.682195</td>\n",
       "      <td>0.681640</td>\n",
       "      <td>0.681171</td>\n",
       "      <td>0.680729</td>\n",
       "      <td>0.680484</td>\n",
       "      <td>0.680928</td>\n",
       "      <td>0.682209</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.684659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.475171</td>\n",
       "      <td>0.406982</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.402194</td>\n",
       "      <td>0.448523</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.274347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.679731</td>\n",
       "      <td>0.678763</td>\n",
       "      <td>0.678157</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.679554</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.681912</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>0.682467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313650</td>\n",
       "      <td>0.273656</td>\n",
       "      <td>0.261276</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.356592</td>\n",
       "      <td>0.389055</td>\n",
       "      <td>0.355631</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.410403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692921</td>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.690065</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.687545</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.685337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346655</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.342483</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.278585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684316</td>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.684634</td>\n",
       "      <td>0.684632</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.683996</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.682420</td>\n",
       "      <td>0.681621</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277664</td>\n",
       "      <td>0.265779</td>\n",
       "      <td>0.276489</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.568265</td>\n",
       "      <td>0.505633</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.407461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.671236</td>\n",
       "      <td>0.670445</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.667360</td>\n",
       "      <td>0.668124</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474643</td>\n",
       "      <td>0.415693</td>\n",
       "      <td>0.381110</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.460601</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.281171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.699013</td>\n",
       "      <td>0.700283</td>\n",
       "      <td>0.701001</td>\n",
       "      <td>0.701229</td>\n",
       "      <td>0.701127</td>\n",
       "      <td>0.700915</td>\n",
       "      <td>0.700716</td>\n",
       "      <td>0.700710</td>\n",
       "      <td>0.700854</td>\n",
       "      <td>0.700750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164359</td>\n",
       "      <td>0.156490</td>\n",
       "      <td>0.160885</td>\n",
       "      <td>0.174962</td>\n",
       "      <td>0.209262</td>\n",
       "      <td>0.245508</td>\n",
       "      <td>0.258957</td>\n",
       "      <td>0.223890</td>\n",
       "      <td>0.225651</td>\n",
       "      <td>0.280271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.693254</td>\n",
       "      <td>0.693763</td>\n",
       "      <td>0.694442</td>\n",
       "      <td>0.695615</td>\n",
       "      <td>0.697754</td>\n",
       "      <td>0.700593</td>\n",
       "      <td>0.703291</td>\n",
       "      <td>0.705338</td>\n",
       "      <td>0.706631</td>\n",
       "      <td>0.707332</td>\n",
       "      <td>...</td>\n",
       "      <td>0.335628</td>\n",
       "      <td>0.430326</td>\n",
       "      <td>0.492045</td>\n",
       "      <td>0.491282</td>\n",
       "      <td>0.453160</td>\n",
       "      <td>0.393129</td>\n",
       "      <td>0.308887</td>\n",
       "      <td>0.198685</td>\n",
       "      <td>0.149238</td>\n",
       "      <td>0.125311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.686958</td>\n",
       "      <td>0.686474</td>\n",
       "      <td>0.685674</td>\n",
       "      <td>0.684757</td>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.683164</td>\n",
       "      <td>0.682842</td>\n",
       "      <td>0.682819</td>\n",
       "      <td>0.683196</td>\n",
       "      <td>0.683973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173705</td>\n",
       "      <td>0.166870</td>\n",
       "      <td>0.171276</td>\n",
       "      <td>0.188868</td>\n",
       "      <td>0.235896</td>\n",
       "      <td>0.324066</td>\n",
       "      <td>0.461164</td>\n",
       "      <td>0.508763</td>\n",
       "      <td>0.510587</td>\n",
       "      <td>0.458503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.652792</td>\n",
       "      <td>0.653381</td>\n",
       "      <td>0.654533</td>\n",
       "      <td>0.656147</td>\n",
       "      <td>0.657785</td>\n",
       "      <td>0.659059</td>\n",
       "      <td>0.659839</td>\n",
       "      <td>0.660204</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.660191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193938</td>\n",
       "      <td>0.162887</td>\n",
       "      <td>0.151263</td>\n",
       "      <td>0.158412</td>\n",
       "      <td>0.186152</td>\n",
       "      <td>0.202066</td>\n",
       "      <td>0.179934</td>\n",
       "      <td>0.118501</td>\n",
       "      <td>0.085939</td>\n",
       "      <td>0.069742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.739684</td>\n",
       "      <td>0.740548</td>\n",
       "      <td>0.741296</td>\n",
       "      <td>0.741795</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.742149</td>\n",
       "      <td>0.742147</td>\n",
       "      <td>0.742174</td>\n",
       "      <td>0.742341</td>\n",
       "      <td>0.742520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348637</td>\n",
       "      <td>0.317461</td>\n",
       "      <td>0.309872</td>\n",
       "      <td>0.324136</td>\n",
       "      <td>0.363829</td>\n",
       "      <td>0.413066</td>\n",
       "      <td>0.431430</td>\n",
       "      <td>0.352381</td>\n",
       "      <td>0.300207</td>\n",
       "      <td>0.265476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_201  current_202  current_203  current_204  current_205  \\\n",
       "0       0.682921     0.682195     0.681640     0.681171     0.680729   \n",
       "1       0.679731     0.678763     0.678157     0.678000     0.678474   \n",
       "2       0.692921     0.692640     0.692082     0.691222     0.690065   \n",
       "3       0.684316     0.684550     0.684634     0.684632     0.684461   \n",
       "4       0.671236     0.670445     0.669661     0.668889     0.668196   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "465     0.699013     0.700283     0.701001     0.701229     0.701127   \n",
       "466     0.693254     0.693763     0.694442     0.695615     0.697754   \n",
       "467     0.686958     0.686474     0.685674     0.684757     0.683837   \n",
       "468     0.652792     0.653381     0.654533     0.656147     0.657785   \n",
       "469     0.739684     0.740548     0.741296     0.741795     0.742056   \n",
       "\n",
       "     current_206  current_207  current_208  current_209  current_210  ...  \\\n",
       "0       0.680484     0.680928     0.682209     0.683694     0.684659  ...   \n",
       "1       0.679554     0.680862     0.681912     0.682436     0.682467  ...   \n",
       "2       0.688761     0.687545     0.686558     0.685798     0.685337  ...   \n",
       "3       0.683996     0.683266     0.682420     0.681621     0.680988  ...   \n",
       "4       0.667722     0.667399     0.667360     0.668124     0.669890  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "465     0.700915     0.700716     0.700710     0.700854     0.700750  ...   \n",
       "466     0.700593     0.703291     0.705338     0.706631     0.707332  ...   \n",
       "467     0.683164     0.682842     0.682819     0.683196     0.683973  ...   \n",
       "468     0.659059     0.659839     0.660204     0.660300     0.660191  ...   \n",
       "469     0.742149     0.742147     0.742174     0.742341     0.742520  ...   \n",
       "\n",
       "     volt_791  volt_792  volt_793  volt_794  volt_795  volt_796  volt_797  \\\n",
       "0    0.475171  0.406982  0.367278  0.364093  0.402194  0.448523  0.450469   \n",
       "1    0.313650  0.273656  0.261276  0.271836  0.308600  0.356592  0.389055   \n",
       "2    0.346655  0.317711  0.297014  0.288137  0.302547  0.330311  0.342483   \n",
       "3    0.277664  0.265779  0.276489  0.310451  0.382017  0.485955  0.568265   \n",
       "4    0.474643  0.415693  0.381110  0.376050  0.407861  0.460601  0.481486   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "465  0.164359  0.156490  0.160885  0.174962  0.209262  0.245508  0.258957   \n",
       "466  0.335628  0.430326  0.492045  0.491282  0.453160  0.393129  0.308887   \n",
       "467  0.173705  0.166870  0.171276  0.188868  0.235896  0.324066  0.461164   \n",
       "468  0.193938  0.162887  0.151263  0.158412  0.186152  0.202066  0.179934   \n",
       "469  0.348637  0.317461  0.309872  0.324136  0.363829  0.413066  0.431430   \n",
       "\n",
       "     volt_798  volt_799  volt_800  \n",
       "0    0.356803  0.302953  0.274347  \n",
       "1    0.355631  0.369090  0.410403  \n",
       "2    0.289035  0.271342  0.278585  \n",
       "3    0.505633  0.450684  0.407461  \n",
       "4    0.389891  0.325563  0.281171  \n",
       "..        ...       ...       ...  \n",
       "465  0.223890  0.225651  0.280271  \n",
       "466  0.198685  0.149238  0.125311  \n",
       "467  0.508763  0.510587  0.458503  \n",
       "468  0.118501  0.085939  0.069742  \n",
       "469  0.352381  0.300207  0.265476  \n",
       "\n",
       "[295 rows x 1200 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_201</th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_791</th>\n",
       "      <th>volt_792</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.691468</td>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.691857</td>\n",
       "      <td>0.691331</td>\n",
       "      <td>0.690484</td>\n",
       "      <td>0.689532</td>\n",
       "      <td>0.688726</td>\n",
       "      <td>0.688130</td>\n",
       "      <td>0.687714</td>\n",
       "      <td>0.687552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403621</td>\n",
       "      <td>0.342177</td>\n",
       "      <td>0.314036</td>\n",
       "      <td>0.318342</td>\n",
       "      <td>0.356268</td>\n",
       "      <td>0.401804</td>\n",
       "      <td>0.407339</td>\n",
       "      <td>0.316755</td>\n",
       "      <td>0.256458</td>\n",
       "      <td>0.221252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.689295</td>\n",
       "      <td>0.688282</td>\n",
       "      <td>0.687451</td>\n",
       "      <td>0.686783</td>\n",
       "      <td>0.686297</td>\n",
       "      <td>0.686016</td>\n",
       "      <td>0.686165</td>\n",
       "      <td>0.687262</td>\n",
       "      <td>0.689330</td>\n",
       "      <td>0.691714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323495</td>\n",
       "      <td>0.293242</td>\n",
       "      <td>0.277765</td>\n",
       "      <td>0.266636</td>\n",
       "      <td>0.272530</td>\n",
       "      <td>0.298734</td>\n",
       "      <td>0.329949</td>\n",
       "      <td>0.307095</td>\n",
       "      <td>0.308849</td>\n",
       "      <td>0.322219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.704625</td>\n",
       "      <td>0.703717</td>\n",
       "      <td>0.703216</td>\n",
       "      <td>0.703154</td>\n",
       "      <td>0.703356</td>\n",
       "      <td>0.703613</td>\n",
       "      <td>0.703891</td>\n",
       "      <td>0.704373</td>\n",
       "      <td>0.705145</td>\n",
       "      <td>0.705897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235213</td>\n",
       "      <td>0.212009</td>\n",
       "      <td>0.232290</td>\n",
       "      <td>0.283721</td>\n",
       "      <td>0.357671</td>\n",
       "      <td>0.427205</td>\n",
       "      <td>0.455655</td>\n",
       "      <td>0.389299</td>\n",
       "      <td>0.366152</td>\n",
       "      <td>0.376777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.702460</td>\n",
       "      <td>0.702957</td>\n",
       "      <td>0.704118</td>\n",
       "      <td>0.705578</td>\n",
       "      <td>0.706766</td>\n",
       "      <td>0.707387</td>\n",
       "      <td>0.707607</td>\n",
       "      <td>0.707736</td>\n",
       "      <td>0.707861</td>\n",
       "      <td>0.707882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455123</td>\n",
       "      <td>0.387404</td>\n",
       "      <td>0.347294</td>\n",
       "      <td>0.338249</td>\n",
       "      <td>0.361233</td>\n",
       "      <td>0.389561</td>\n",
       "      <td>0.377679</td>\n",
       "      <td>0.282076</td>\n",
       "      <td>0.228494</td>\n",
       "      <td>0.216340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.710469</td>\n",
       "      <td>0.708822</td>\n",
       "      <td>0.707448</td>\n",
       "      <td>0.706760</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.706796</td>\n",
       "      <td>0.706901</td>\n",
       "      <td>0.706971</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306752</td>\n",
       "      <td>0.308216</td>\n",
       "      <td>0.310868</td>\n",
       "      <td>0.323062</td>\n",
       "      <td>0.364767</td>\n",
       "      <td>0.443387</td>\n",
       "      <td>0.525665</td>\n",
       "      <td>0.484873</td>\n",
       "      <td>0.442928</td>\n",
       "      <td>0.400778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.655160</td>\n",
       "      <td>0.654703</td>\n",
       "      <td>0.654392</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>0.654086</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.655265</td>\n",
       "      <td>0.656777</td>\n",
       "      <td>0.658473</td>\n",
       "      <td>0.659922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118411</td>\n",
       "      <td>0.085348</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>0.079646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.688720</td>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.693366</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.696822</td>\n",
       "      <td>0.698040</td>\n",
       "      <td>0.699004</td>\n",
       "      <td>0.699738</td>\n",
       "      <td>0.700179</td>\n",
       "      <td>0.700178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267515</td>\n",
       "      <td>0.243962</td>\n",
       "      <td>0.250543</td>\n",
       "      <td>0.279990</td>\n",
       "      <td>0.333953</td>\n",
       "      <td>0.393716</td>\n",
       "      <td>0.419058</td>\n",
       "      <td>0.355549</td>\n",
       "      <td>0.327488</td>\n",
       "      <td>0.312347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.717838</td>\n",
       "      <td>0.716938</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.714911</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.712956</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>0.710904</td>\n",
       "      <td>0.709947</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300854</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>0.262787</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.268023</td>\n",
       "      <td>0.270470</td>\n",
       "      <td>0.250558</td>\n",
       "      <td>0.187487</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>0.156544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.671228</td>\n",
       "      <td>0.671696</td>\n",
       "      <td>0.671971</td>\n",
       "      <td>0.672160</td>\n",
       "      <td>0.672397</td>\n",
       "      <td>0.672658</td>\n",
       "      <td>0.672790</td>\n",
       "      <td>0.672632</td>\n",
       "      <td>0.672193</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149968</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.139477</td>\n",
       "      <td>0.127960</td>\n",
       "      <td>0.122262</td>\n",
       "      <td>0.118875</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.071337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.722550</td>\n",
       "      <td>0.721452</td>\n",
       "      <td>0.720337</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.716881</td>\n",
       "      <td>0.715988</td>\n",
       "      <td>0.715285</td>\n",
       "      <td>0.714784</td>\n",
       "      <td>0.714635</td>\n",
       "      <td>...</td>\n",
       "      <td>0.169253</td>\n",
       "      <td>0.158205</td>\n",
       "      <td>0.208111</td>\n",
       "      <td>0.335710</td>\n",
       "      <td>0.476410</td>\n",
       "      <td>0.531009</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.308952</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.165598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_201  current_202  current_203  current_204  current_205  \\\n",
       "125     0.691468     0.691919     0.691857     0.691331     0.690484   \n",
       "126     0.689295     0.688282     0.687451     0.686783     0.686297   \n",
       "127     0.704625     0.703717     0.703216     0.703154     0.703356   \n",
       "128     0.702460     0.702957     0.704118     0.705578     0.706766   \n",
       "129     0.711864     0.710469     0.708822     0.707448     0.706760   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "474     0.655160     0.654703     0.654392     0.654158     0.654086   \n",
       "475     0.688720     0.691066     0.693366     0.695296     0.696822   \n",
       "476     0.717838     0.716938     0.715976     0.714911     0.713888   \n",
       "477     0.671228     0.671696     0.671971     0.672160     0.672397   \n",
       "478     0.722550     0.721452     0.720337     0.719178     0.717978   \n",
       "\n",
       "     current_206  current_207  current_208  current_209  current_210  ...  \\\n",
       "125     0.689532     0.688726     0.688130     0.687714     0.687552  ...   \n",
       "126     0.686016     0.686165     0.687262     0.689330     0.691714  ...   \n",
       "127     0.703613     0.703891     0.704373     0.705145     0.705897  ...   \n",
       "128     0.707387     0.707607     0.707736     0.707861     0.707882  ...   \n",
       "129     0.706676     0.706796     0.706901     0.706971     0.707142  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "474     0.654370     0.655265     0.656777     0.658473     0.659922  ...   \n",
       "475     0.698040     0.699004     0.699738     0.700179     0.700178  ...   \n",
       "476     0.712956     0.711976     0.710904     0.709947     0.709364  ...   \n",
       "477     0.672658     0.672790     0.672632     0.672193     0.671623  ...   \n",
       "478     0.716881     0.715988     0.715285     0.714784     0.714635  ...   \n",
       "\n",
       "     volt_791  volt_792  volt_793  volt_794  volt_795  volt_796  volt_797  \\\n",
       "125  0.403621  0.342177  0.314036  0.318342  0.356268  0.401804  0.407339   \n",
       "126  0.323495  0.293242  0.277765  0.266636  0.272530  0.298734  0.329949   \n",
       "127  0.235213  0.212009  0.232290  0.283721  0.357671  0.427205  0.455655   \n",
       "128  0.455123  0.387404  0.347294  0.338249  0.361233  0.389561  0.377679   \n",
       "129  0.306752  0.308216  0.310868  0.323062  0.364767  0.443387  0.525665   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "474  0.118411  0.085348  0.067686  0.062863  0.078773  0.102914  0.117693   \n",
       "475  0.267515  0.243962  0.250543  0.279990  0.333953  0.393716  0.419058   \n",
       "476  0.300854  0.269906  0.262787  0.261464  0.268023  0.270470  0.250558   \n",
       "477  0.149968  0.143442  0.139477  0.127960  0.122262  0.118875  0.110584   \n",
       "478  0.169253  0.158205  0.208111  0.335710  0.476410  0.531009  0.470152   \n",
       "\n",
       "     volt_798  volt_799  volt_800  \n",
       "125  0.316755  0.256458  0.221252  \n",
       "126  0.307095  0.308849  0.322219  \n",
       "127  0.389299  0.366152  0.376777  \n",
       "128  0.282076  0.228494  0.216340  \n",
       "129  0.484873  0.442928  0.400778  \n",
       "..        ...       ...       ...  \n",
       "474  0.097859  0.085690  0.079646  \n",
       "475  0.355549  0.327488  0.312347  \n",
       "476  0.187487  0.160850  0.156544  \n",
       "477  0.083422  0.073359  0.071337  \n",
       "478  0.308952  0.213821  0.165598  \n",
       "\n",
       "[184 rows x 1200 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = True\n",
    "ann = MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 300),\n",
    "    max_iter=300,\n",
    "    tol=0.0000000001,\n",
    "    learning_rate_init=0.01,\n",
    "    solver='adam',\n",
    "    activation='relu',\n",
    "    learning_rate='constant',\n",
    "    verbose=True,\n",
    "    early_stopping=early_stopping, # needed to get validation_scores stats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.14859499\n",
      "Validation score: 0.333333\n",
      "Iteration 2, loss = 12.69375562\n",
      "Validation score: 0.633333\n",
      "Iteration 3, loss = 2.53643968\n",
      "Validation score: 0.900000\n",
      "Iteration 4, loss = 0.51297459\n",
      "Validation score: 0.900000\n",
      "Iteration 5, loss = 0.39467137\n",
      "Validation score: 0.733333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.44178437\n",
      "Validation score: 0.933333\n",
      "Iteration 7, loss = 0.34550975\n",
      "Validation score: 0.900000\n",
      "Iteration 8, loss = 0.33149023\n",
      "Validation score: 0.900000\n",
      "Iteration 9, loss = 0.26637152\n",
      "Validation score: 0.766667\n",
      "Iteration 10, loss = 0.36714507\n",
      "Validation score: 0.900000\n",
      "Iteration 11, loss = 0.32310378\n",
      "Validation score: 0.866667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, loss = 0.29353481\n",
      "Validation score: 0.733333\n",
      "Iteration 13, loss = 0.50301102\n",
      "Validation score: 0.900000\n",
      "Iteration 14, loss = 0.23952040\n",
      "Validation score: 0.933333\n",
      "Iteration 15, loss = 0.18403779\n",
      "Validation score: 0.900000\n",
      "Iteration 16, loss = 0.16675498\n",
      "Validation score: 0.966667\n",
      "Iteration 17, loss = 0.11739812\n",
      "Validation score: 0.966667\n",
      "Iteration 18, loss = 0.10974241\n",
      "Validation score: 1.000000\n",
      "Iteration 19, loss = 0.10713124\n",
      "Validation score: 1.000000\n",
      "Iteration 20, loss = 0.08414624\n",
      "Validation score: 0.966667\n",
      "Iteration 21, loss = 0.08346862\n",
      "Validation score: 1.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 22, loss = 0.07489200\n",
      "Validation score: 1.000000\n",
      "Iteration 23, loss = 0.06866559\n",
      "Validation score: 1.000000\n",
      "Iteration 24, loss = 0.06404727\n",
      "Validation score: 1.000000\n",
      "Iteration 25, loss = 0.06585295\n",
      "Validation score: 1.000000\n",
      "Iteration 26, loss = 0.06090943\n",
      "Validation score: 1.000000\n",
      "Iteration 27, loss = 0.05292313\n",
      "Validation score: 1.000000\n",
      "Iteration 28, loss = 0.05950707\n",
      "Validation score: 1.000000\n",
      "Iteration 29, loss = 0.05066426\n",
      "Validation score: 1.000000\n",
      "Validation score did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but MLPClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(early_stopping=True, hidden_layer_sizes=(150, 300),\n",
       "              learning_rate_init=0.01, max_iter=300, tol=1e-10, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(early_stopping=True, hidden_layer_sizes=(150, 300),\n",
       "              learning_rate_init=0.01, max_iter=300, tol=1e-10, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(early_stopping=True, hidden_layer_sizes=(150, 300),\n",
       "              learning_rate_init=0.01, max_iter=300, tol=1e-10, verbose=True)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the model is well fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if early_stopping:\n",
    "    loss_train = ann.loss_curve_\n",
    "    loss_validation = ann.validation_scores_\n",
    "\n",
    "    loss_train_df = pd.DataFrame(\n",
    "        {\n",
    "            'loss': loss_train,\n",
    "            'iterations': [x for x in range(len(loss_train))]\n",
    "        }\n",
    "    )\n",
    "    loss_train_df['Data'] = 'Training'\n",
    "\n",
    "    loss_validation_df = pd.DataFrame(\n",
    "        {\n",
    "            'loss': [1 - x for x in loss_validation],\n",
    "            'iterations': [x for x in range(len(loss_validation))]\n",
    "        }\n",
    "    )\n",
    "    loss_validation_df['Data'] = 'Validation'\n",
    "\n",
    "    loss_df = pd.concat([loss_train_df, loss_validation_df])\n",
    "\n",
    "    sns.lineplot(data=loss_df, x='iterations', y='loss', hue='Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot showed that the model is well fitted as the loss from validation and training datasets are nearly the same in the end of fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.46%\n"
     ]
    }
   ],
   "source": [
    "score = ann.score(X_test, y_test)\n",
    "y_pred = ann.predict(X_test)\n",
    "\n",
    "\n",
    "print('%.2f' %(score*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# Learn to predict each class against the other\n",
    "\n",
    "\n",
    "n_classes = 3 # number of class\n",
    "\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i], )\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the probability for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a random sample of state 0: [[0.9053 0.     0.0947]]\n",
      "Probability of a random sample of state 1: [[0. 1. 0.]]\n",
      "Probability of a random sample of state 2: [[0.0423 0.0023 0.9554]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of a random sample of state 0:\", np.round(ann.predict_proba(X_test_0[:1]), 4) )\n",
    "print(\"Probability of a random sample of state 1:\", np.round(ann.predict_proba(X_test_1[:1]), 4) )\n",
    "print(\"Probability of a random sample of state 2:\", np.round(ann.predict_proba(X_test_2[:1]), 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN Characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'alpha': 0.0001,\n",
       " 'batch_size': 'auto',\n",
       " 'beta_1': 0.9,\n",
       " 'beta_2': 0.999,\n",
       " 'early_stopping': True,\n",
       " 'epsilon': 1e-08,\n",
       " 'hidden_layer_sizes': (150, 300),\n",
       " 'learning_rate': 'constant',\n",
       " 'learning_rate_init': 0.01,\n",
       " 'max_fun': 15000,\n",
       " 'max_iter': 300,\n",
       " 'momentum': 0.9,\n",
       " 'n_iter_no_change': 10,\n",
       " 'nesterovs_momentum': True,\n",
       " 'power_t': 0.5,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'solver': 'adam',\n",
       " 'tol': 1e-10,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': True,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([234, 182, 154, 223, 293, 193, 270, 150,  27, 242,\\n            ...\\n             93, 139,  35, 204,  63,  29,   0, 267,  78,  66],\\n           dtype='int64', length=128)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[39m# MINI-BATCH\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     indices \u001b[39m=\u001b[39m random_perm[mini_batch_index:mini_batch_index \u001b[39m+\u001b[39m N_BATCH]\n\u001b[0;32m---> 27\u001b[0m     ann\u001b[39m.\u001b[39mpartial_fit(X_train[indices], y_train[indices], classes\u001b[39m=\u001b[39mN_CLASSES)\n\u001b[1;32m     28\u001b[0m     mini_batch_index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m N_BATCH\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m mini_batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m N_TRAIN_SAMPLES:\n",
      "File \u001b[0;32m~/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6117\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:6173\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6171\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6172\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 6173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   6176\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([234, 182, 154, 223, 293, 193, 270, 150,  27, 242,\\n            ...\\n             93, 139,  35, 204,  63,  29,   0, 267,  78,  66],\\n           dtype='int64', length=128)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "np.random.seed(1)\n",
    "\n",
    "\"\"\" Home-made mini-batch learning\n",
    "    -> not to be used in out-of-core setting!\n",
    "\"\"\"\n",
    "N_TRAIN_SAMPLES = X_train.shape[0]\n",
    "N_EPOCHS = 25\n",
    "N_BATCH = 128\n",
    "N_CLASSES = np.unique(y_train)\n",
    "\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "# EPOCH\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    print('epoch: ', epoch)\n",
    "    # SHUFFLING\n",
    "    random_perm = np.random.permutation(X_train.shape[0])\n",
    "    mini_batch_index = 0\n",
    "    while True:\n",
    "        # MINI-BATCH\n",
    "        indices = random_perm[mini_batch_index:mini_batch_index + N_BATCH]\n",
    "        ann.partial_fit(X_train[indices], y_train[indices], classes=N_CLASSES)\n",
    "        mini_batch_index += N_BATCH\n",
    "\n",
    "        if mini_batch_index >= N_TRAIN_SAMPLES:\n",
    "            break\n",
    "\n",
    "    # SCORE TRAIN\n",
    "    scores_train.append(ann.score(X_train, y_train))\n",
    "\n",
    "    # SCORE TEST\n",
    "    scores_test.append(ann.score(X_test, y_test))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "plt.plot(scores_train, color='green', alpha=0.8, label='Train')\n",
    "plt.plot(scores_test, color='magenta', alpha=0.8, label='Test')\n",
    "plt.title(\"Accuracy over epochs\", fontsize=14)\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c00d594dade041c07c65151f11049b96387fcc7bcfd07780fd4f8feb49b14c80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
