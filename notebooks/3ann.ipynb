{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network Build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build an artificial neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>current_211</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "      <th>volt_801</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682195</td>\n",
       "      <td>0.681640</td>\n",
       "      <td>0.681171</td>\n",
       "      <td>0.680729</td>\n",
       "      <td>0.680484</td>\n",
       "      <td>0.680928</td>\n",
       "      <td>0.682209</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.684659</td>\n",
       "      <td>0.684947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.402194</td>\n",
       "      <td>0.448523</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.274347</td>\n",
       "      <td>0.187275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.678763</td>\n",
       "      <td>0.678157</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.679554</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.681912</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>0.682467</td>\n",
       "      <td>0.682152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261276</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.356592</td>\n",
       "      <td>0.389055</td>\n",
       "      <td>0.355631</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.323149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.690065</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.687545</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.685337</td>\n",
       "      <td>0.685325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.342483</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.278585</td>\n",
       "      <td>0.217022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.684634</td>\n",
       "      <td>0.684632</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.683996</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.682420</td>\n",
       "      <td>0.681621</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>0.680663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276489</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.568265</td>\n",
       "      <td>0.505633</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.407461</td>\n",
       "      <td>0.272598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670445</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.667360</td>\n",
       "      <td>0.668124</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381110</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.460601</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.281171</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   current_202  current_203  current_204  current_205  current_206  \\\n",
       "0     0.682195     0.681640     0.681171     0.680729     0.680484   \n",
       "1     0.678763     0.678157     0.678000     0.678474     0.679554   \n",
       "2     0.692640     0.692082     0.691222     0.690065     0.688761   \n",
       "3     0.684550     0.684634     0.684632     0.684461     0.683996   \n",
       "4     0.670445     0.669661     0.668889     0.668196     0.667722   \n",
       "\n",
       "   current_207  current_208  current_209  current_210  current_211  ...  \\\n",
       "0     0.680928     0.682209     0.683694     0.684659     0.684947  ...   \n",
       "1     0.680862     0.681912     0.682436     0.682467     0.682152  ...   \n",
       "2     0.687545     0.686558     0.685798     0.685337     0.685325  ...   \n",
       "3     0.683266     0.682420     0.681621     0.680988     0.680663  ...   \n",
       "4     0.667399     0.667360     0.668124     0.669890     0.672200  ...   \n",
       "\n",
       "   volt_793  volt_794  volt_795  volt_796  volt_797  volt_798  volt_799  \\\n",
       "0  0.367278  0.364093  0.402194  0.448523  0.450469  0.356803  0.302953   \n",
       "1  0.261276  0.271836  0.308600  0.356592  0.389055  0.355631  0.369090   \n",
       "2  0.297014  0.288137  0.302547  0.330311  0.342483  0.289035  0.271342   \n",
       "3  0.276489  0.310451  0.382017  0.485955  0.568265  0.505633  0.450684   \n",
       "4  0.381110  0.376050  0.407861  0.460601  0.481486  0.389891  0.325563   \n",
       "\n",
       "   volt_800  volt_801  state  \n",
       "0  0.274347  0.187275      0  \n",
       "1  0.410403  0.323149      0  \n",
       "2  0.278585  0.217022      0  \n",
       "3  0.407461  0.272598      0  \n",
       "4  0.281171  0.180556      0  \n",
       "\n",
       "[5 rows x 1201 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/dataset_cleaned.csv'\n",
    "data = pd.read_csv('../data/dataset_cleaned.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>current_211</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_792</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "      <th>volt_801</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682195</td>\n",
       "      <td>0.681640</td>\n",
       "      <td>0.681171</td>\n",
       "      <td>0.680729</td>\n",
       "      <td>0.680484</td>\n",
       "      <td>0.680928</td>\n",
       "      <td>0.682209</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.684659</td>\n",
       "      <td>0.684947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406982</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.402194</td>\n",
       "      <td>0.448523</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.274347</td>\n",
       "      <td>0.187275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.678763</td>\n",
       "      <td>0.678157</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.679554</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.681912</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>0.682467</td>\n",
       "      <td>0.682152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273656</td>\n",
       "      <td>0.261276</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.356592</td>\n",
       "      <td>0.389055</td>\n",
       "      <td>0.355631</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.323149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.690065</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.687545</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.685337</td>\n",
       "      <td>0.685325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.317711</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.342483</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.278585</td>\n",
       "      <td>0.217022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.684634</td>\n",
       "      <td>0.684632</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.683996</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.682420</td>\n",
       "      <td>0.681621</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>0.680663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265779</td>\n",
       "      <td>0.276489</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.568265</td>\n",
       "      <td>0.505633</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.407461</td>\n",
       "      <td>0.272598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670445</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.667360</td>\n",
       "      <td>0.668124</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415693</td>\n",
       "      <td>0.381110</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.460601</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.281171</td>\n",
       "      <td>0.180556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.654703</td>\n",
       "      <td>0.654392</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>0.654086</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.655265</td>\n",
       "      <td>0.656777</td>\n",
       "      <td>0.658473</td>\n",
       "      <td>0.659922</td>\n",
       "      <td>0.661014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085348</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.061386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.693366</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.696822</td>\n",
       "      <td>0.698040</td>\n",
       "      <td>0.699004</td>\n",
       "      <td>0.699738</td>\n",
       "      <td>0.700179</td>\n",
       "      <td>0.700178</td>\n",
       "      <td>0.699792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243962</td>\n",
       "      <td>0.250543</td>\n",
       "      <td>0.279990</td>\n",
       "      <td>0.333953</td>\n",
       "      <td>0.393716</td>\n",
       "      <td>0.419058</td>\n",
       "      <td>0.355549</td>\n",
       "      <td>0.327488</td>\n",
       "      <td>0.312347</td>\n",
       "      <td>0.211438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.716938</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.714911</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.712956</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>0.710904</td>\n",
       "      <td>0.709947</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.709295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269906</td>\n",
       "      <td>0.262787</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.268023</td>\n",
       "      <td>0.270470</td>\n",
       "      <td>0.250558</td>\n",
       "      <td>0.187487</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>0.156544</td>\n",
       "      <td>0.107699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.671696</td>\n",
       "      <td>0.671971</td>\n",
       "      <td>0.672160</td>\n",
       "      <td>0.672397</td>\n",
       "      <td>0.672658</td>\n",
       "      <td>0.672790</td>\n",
       "      <td>0.672632</td>\n",
       "      <td>0.672193</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>0.670950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.139477</td>\n",
       "      <td>0.127960</td>\n",
       "      <td>0.122262</td>\n",
       "      <td>0.118875</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>0.047548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.721452</td>\n",
       "      <td>0.720337</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.716881</td>\n",
       "      <td>0.715988</td>\n",
       "      <td>0.715285</td>\n",
       "      <td>0.714784</td>\n",
       "      <td>0.714635</td>\n",
       "      <td>0.714874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158205</td>\n",
       "      <td>0.208111</td>\n",
       "      <td>0.335710</td>\n",
       "      <td>0.476410</td>\n",
       "      <td>0.531009</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.308952</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.165598</td>\n",
       "      <td>0.098637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479 rows Ã— 1200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_202  current_203  current_204  current_205  current_206  \\\n",
       "0       0.682195     0.681640     0.681171     0.680729     0.680484   \n",
       "1       0.678763     0.678157     0.678000     0.678474     0.679554   \n",
       "2       0.692640     0.692082     0.691222     0.690065     0.688761   \n",
       "3       0.684550     0.684634     0.684632     0.684461     0.683996   \n",
       "4       0.670445     0.669661     0.668889     0.668196     0.667722   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "474     0.654703     0.654392     0.654158     0.654086     0.654370   \n",
       "475     0.691066     0.693366     0.695296     0.696822     0.698040   \n",
       "476     0.716938     0.715976     0.714911     0.713888     0.712956   \n",
       "477     0.671696     0.671971     0.672160     0.672397     0.672658   \n",
       "478     0.721452     0.720337     0.719178     0.717978     0.716881   \n",
       "\n",
       "     current_207  current_208  current_209  current_210  current_211  ...  \\\n",
       "0       0.680928     0.682209     0.683694     0.684659     0.684947  ...   \n",
       "1       0.680862     0.681912     0.682436     0.682467     0.682152  ...   \n",
       "2       0.687545     0.686558     0.685798     0.685337     0.685325  ...   \n",
       "3       0.683266     0.682420     0.681621     0.680988     0.680663  ...   \n",
       "4       0.667399     0.667360     0.668124     0.669890     0.672200  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "474     0.655265     0.656777     0.658473     0.659922     0.661014  ...   \n",
       "475     0.699004     0.699738     0.700179     0.700178     0.699792  ...   \n",
       "476     0.711976     0.710904     0.709947     0.709364     0.709295  ...   \n",
       "477     0.672790     0.672632     0.672193     0.671623     0.670950  ...   \n",
       "478     0.715988     0.715285     0.714784     0.714635     0.714874  ...   \n",
       "\n",
       "     volt_792  volt_793  volt_794  volt_795  volt_796  volt_797  volt_798  \\\n",
       "0    0.406982  0.367278  0.364093  0.402194  0.448523  0.450469  0.356803   \n",
       "1    0.273656  0.261276  0.271836  0.308600  0.356592  0.389055  0.355631   \n",
       "2    0.317711  0.297014  0.288137  0.302547  0.330311  0.342483  0.289035   \n",
       "3    0.265779  0.276489  0.310451  0.382017  0.485955  0.568265  0.505633   \n",
       "4    0.415693  0.381110  0.376050  0.407861  0.460601  0.481486  0.389891   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "474  0.085348  0.067686  0.062863  0.078773  0.102914  0.117693  0.097859   \n",
       "475  0.243962  0.250543  0.279990  0.333953  0.393716  0.419058  0.355549   \n",
       "476  0.269906  0.262787  0.261464  0.268023  0.270470  0.250558  0.187487   \n",
       "477  0.143442  0.139477  0.127960  0.122262  0.118875  0.110584  0.083422   \n",
       "478  0.158205  0.208111  0.335710  0.476410  0.531009  0.470152  0.308952   \n",
       "\n",
       "     volt_799  volt_800  volt_801  \n",
       "0    0.302953  0.274347  0.187275  \n",
       "1    0.369090  0.410403  0.323149  \n",
       "2    0.271342  0.278585  0.217022  \n",
       "3    0.450684  0.407461  0.272598  \n",
       "4    0.325563  0.281171  0.180556  \n",
       "..        ...       ...       ...  \n",
       "474  0.085690  0.079646  0.061386  \n",
       "475  0.327488  0.312347  0.211438  \n",
       "476  0.160850  0.156544  0.107699  \n",
       "477  0.073359  0.071337  0.047548  \n",
       "478  0.213821  0.165598  0.098637  \n",
       "\n",
       "[479 rows x 1200 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the best training and testing datasets, we're going to balance the amount of each state.\n",
    "We're going to use:\n",
    "\n",
    "Training:\n",
    "120 from state 0\n",
    "85 from state 1\n",
    "85 from state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 0 shape: (288, 1201)\n",
      "State 1 shape: (97, 1201)\n",
      "State 2 shape: (94, 1201)\n"
     ]
    }
   ],
   "source": [
    "df_0 = data.query('state == 0')\n",
    "df_1 = data.query('state == 1')\n",
    "df_2 = data.query('state == 2')\n",
    "\n",
    "print('State 0 shape:', df_0.shape)\n",
    "print('State 1 shape:', df_1.shape)\n",
    "print('State 2 shape:', df_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape (184, 1201)\n",
      "y_test shape (184,)\n",
      "==========\n",
      "X_train shape (295, 1201)\n",
      "y_train shape (295,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "470    2\n",
       "471    2\n",
       "472    2\n",
       "473    2\n",
       "474    2\n",
       "475    2\n",
       "476    2\n",
       "477    2\n",
       "478    2\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state0_train_size = 125\n",
    "state1_train_size = 85\n",
    "state2_train_size = 85\n",
    "\n",
    "# state 0\n",
    "X_train_0 = df_0.iloc[:state0_train_size, :]\n",
    "X_test_0 = df_0.iloc[state0_train_size:, :]\n",
    "\n",
    "y_train_0 = df_0.iloc[:state0_train_size, -1]\n",
    "y_test_0 = df_0.iloc[state0_train_size:, -1]\n",
    "\n",
    "# state 1\n",
    "X_train_1 = df_1.iloc[:state1_train_size, :]\n",
    "X_test_1 = df_1.iloc[state1_train_size:, :]\n",
    "\n",
    "y_train_1 = df_1.iloc[:state1_train_size, -1]\n",
    "y_test_1 = df_1.iloc[state1_train_size:, -1]\n",
    "\n",
    "# state 2\n",
    "X_train_2 = df_2.iloc[:state2_train_size, :]\n",
    "X_test_2 = df_2.iloc[state2_train_size:, :]\n",
    "\n",
    "y_train_2 = df_2.iloc[:state2_train_size, -1]\n",
    "y_test_2 = df_2.iloc[state2_train_size:, -1]\n",
    "\n",
    "# Concatenate\n",
    "X_train = pd.concat([X_train_0, X_train_1, X_train_2])\n",
    "X_test = pd.concat([X_test_0, X_test_1, X_test_2])\n",
    "\n",
    "y_train = pd.concat([y_train_0, y_train_1, y_train_2])\n",
    "y_test = pd.concat([y_test_0, y_test_1, y_test_2])\n",
    "\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "print(\"==========\")\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "\n",
    "y_test_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>current_211</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "      <th>volt_801</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.682195</td>\n",
       "      <td>0.681640</td>\n",
       "      <td>0.681171</td>\n",
       "      <td>0.680729</td>\n",
       "      <td>0.680484</td>\n",
       "      <td>0.680928</td>\n",
       "      <td>0.682209</td>\n",
       "      <td>0.683694</td>\n",
       "      <td>0.684659</td>\n",
       "      <td>0.684947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367278</td>\n",
       "      <td>0.364093</td>\n",
       "      <td>0.402194</td>\n",
       "      <td>0.448523</td>\n",
       "      <td>0.450469</td>\n",
       "      <td>0.356803</td>\n",
       "      <td>0.302953</td>\n",
       "      <td>0.274347</td>\n",
       "      <td>0.187275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.678763</td>\n",
       "      <td>0.678157</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.678474</td>\n",
       "      <td>0.679554</td>\n",
       "      <td>0.680862</td>\n",
       "      <td>0.681912</td>\n",
       "      <td>0.682436</td>\n",
       "      <td>0.682467</td>\n",
       "      <td>0.682152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261276</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.308600</td>\n",
       "      <td>0.356592</td>\n",
       "      <td>0.389055</td>\n",
       "      <td>0.355631</td>\n",
       "      <td>0.369090</td>\n",
       "      <td>0.410403</td>\n",
       "      <td>0.323149</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.692082</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.690065</td>\n",
       "      <td>0.688761</td>\n",
       "      <td>0.687545</td>\n",
       "      <td>0.686558</td>\n",
       "      <td>0.685798</td>\n",
       "      <td>0.685337</td>\n",
       "      <td>0.685325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297014</td>\n",
       "      <td>0.288137</td>\n",
       "      <td>0.302547</td>\n",
       "      <td>0.330311</td>\n",
       "      <td>0.342483</td>\n",
       "      <td>0.289035</td>\n",
       "      <td>0.271342</td>\n",
       "      <td>0.278585</td>\n",
       "      <td>0.217022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.684634</td>\n",
       "      <td>0.684632</td>\n",
       "      <td>0.684461</td>\n",
       "      <td>0.683996</td>\n",
       "      <td>0.683266</td>\n",
       "      <td>0.682420</td>\n",
       "      <td>0.681621</td>\n",
       "      <td>0.680988</td>\n",
       "      <td>0.680663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276489</td>\n",
       "      <td>0.310451</td>\n",
       "      <td>0.382017</td>\n",
       "      <td>0.485955</td>\n",
       "      <td>0.568265</td>\n",
       "      <td>0.505633</td>\n",
       "      <td>0.450684</td>\n",
       "      <td>0.407461</td>\n",
       "      <td>0.272598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.670445</td>\n",
       "      <td>0.669661</td>\n",
       "      <td>0.668889</td>\n",
       "      <td>0.668196</td>\n",
       "      <td>0.667722</td>\n",
       "      <td>0.667399</td>\n",
       "      <td>0.667360</td>\n",
       "      <td>0.668124</td>\n",
       "      <td>0.669890</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.381110</td>\n",
       "      <td>0.376050</td>\n",
       "      <td>0.407861</td>\n",
       "      <td>0.460601</td>\n",
       "      <td>0.481486</td>\n",
       "      <td>0.389891</td>\n",
       "      <td>0.325563</td>\n",
       "      <td>0.281171</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>0.700283</td>\n",
       "      <td>0.701001</td>\n",
       "      <td>0.701229</td>\n",
       "      <td>0.701127</td>\n",
       "      <td>0.700915</td>\n",
       "      <td>0.700716</td>\n",
       "      <td>0.700710</td>\n",
       "      <td>0.700854</td>\n",
       "      <td>0.700750</td>\n",
       "      <td>0.700235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160885</td>\n",
       "      <td>0.174962</td>\n",
       "      <td>0.209262</td>\n",
       "      <td>0.245508</td>\n",
       "      <td>0.258957</td>\n",
       "      <td>0.223890</td>\n",
       "      <td>0.225651</td>\n",
       "      <td>0.280271</td>\n",
       "      <td>0.285810</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0.693763</td>\n",
       "      <td>0.694442</td>\n",
       "      <td>0.695615</td>\n",
       "      <td>0.697754</td>\n",
       "      <td>0.700593</td>\n",
       "      <td>0.703291</td>\n",
       "      <td>0.705338</td>\n",
       "      <td>0.706631</td>\n",
       "      <td>0.707332</td>\n",
       "      <td>0.707802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492045</td>\n",
       "      <td>0.491282</td>\n",
       "      <td>0.453160</td>\n",
       "      <td>0.393129</td>\n",
       "      <td>0.308887</td>\n",
       "      <td>0.198685</td>\n",
       "      <td>0.149238</td>\n",
       "      <td>0.125311</td>\n",
       "      <td>0.074611</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>0.686474</td>\n",
       "      <td>0.685674</td>\n",
       "      <td>0.684757</td>\n",
       "      <td>0.683837</td>\n",
       "      <td>0.683164</td>\n",
       "      <td>0.682842</td>\n",
       "      <td>0.682819</td>\n",
       "      <td>0.683196</td>\n",
       "      <td>0.683973</td>\n",
       "      <td>0.684908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.171276</td>\n",
       "      <td>0.188868</td>\n",
       "      <td>0.235896</td>\n",
       "      <td>0.324066</td>\n",
       "      <td>0.461164</td>\n",
       "      <td>0.508763</td>\n",
       "      <td>0.510587</td>\n",
       "      <td>0.458503</td>\n",
       "      <td>0.272390</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.653381</td>\n",
       "      <td>0.654533</td>\n",
       "      <td>0.656147</td>\n",
       "      <td>0.657785</td>\n",
       "      <td>0.659059</td>\n",
       "      <td>0.659839</td>\n",
       "      <td>0.660204</td>\n",
       "      <td>0.660300</td>\n",
       "      <td>0.660191</td>\n",
       "      <td>0.659892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151263</td>\n",
       "      <td>0.158412</td>\n",
       "      <td>0.186152</td>\n",
       "      <td>0.202066</td>\n",
       "      <td>0.179934</td>\n",
       "      <td>0.118501</td>\n",
       "      <td>0.085939</td>\n",
       "      <td>0.069742</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0.740548</td>\n",
       "      <td>0.741296</td>\n",
       "      <td>0.741795</td>\n",
       "      <td>0.742056</td>\n",
       "      <td>0.742149</td>\n",
       "      <td>0.742147</td>\n",
       "      <td>0.742174</td>\n",
       "      <td>0.742341</td>\n",
       "      <td>0.742520</td>\n",
       "      <td>0.742521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309872</td>\n",
       "      <td>0.324136</td>\n",
       "      <td>0.363829</td>\n",
       "      <td>0.413066</td>\n",
       "      <td>0.431430</td>\n",
       "      <td>0.352381</td>\n",
       "      <td>0.300207</td>\n",
       "      <td>0.265476</td>\n",
       "      <td>0.173179</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows Ã— 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_202  current_203  current_204  current_205  current_206  \\\n",
       "0       0.682195     0.681640     0.681171     0.680729     0.680484   \n",
       "1       0.678763     0.678157     0.678000     0.678474     0.679554   \n",
       "2       0.692640     0.692082     0.691222     0.690065     0.688761   \n",
       "3       0.684550     0.684634     0.684632     0.684461     0.683996   \n",
       "4       0.670445     0.669661     0.668889     0.668196     0.667722   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "465     0.700283     0.701001     0.701229     0.701127     0.700915   \n",
       "466     0.693763     0.694442     0.695615     0.697754     0.700593   \n",
       "467     0.686474     0.685674     0.684757     0.683837     0.683164   \n",
       "468     0.653381     0.654533     0.656147     0.657785     0.659059   \n",
       "469     0.740548     0.741296     0.741795     0.742056     0.742149   \n",
       "\n",
       "     current_207  current_208  current_209  current_210  current_211  ...  \\\n",
       "0       0.680928     0.682209     0.683694     0.684659     0.684947  ...   \n",
       "1       0.680862     0.681912     0.682436     0.682467     0.682152  ...   \n",
       "2       0.687545     0.686558     0.685798     0.685337     0.685325  ...   \n",
       "3       0.683266     0.682420     0.681621     0.680988     0.680663  ...   \n",
       "4       0.667399     0.667360     0.668124     0.669890     0.672200  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "465     0.700716     0.700710     0.700854     0.700750     0.700235  ...   \n",
       "466     0.703291     0.705338     0.706631     0.707332     0.707802  ...   \n",
       "467     0.682842     0.682819     0.683196     0.683973     0.684908  ...   \n",
       "468     0.659839     0.660204     0.660300     0.660191     0.659892  ...   \n",
       "469     0.742147     0.742174     0.742341     0.742520     0.742521  ...   \n",
       "\n",
       "     volt_793  volt_794  volt_795  volt_796  volt_797  volt_798  volt_799  \\\n",
       "0    0.367278  0.364093  0.402194  0.448523  0.450469  0.356803  0.302953   \n",
       "1    0.261276  0.271836  0.308600  0.356592  0.389055  0.355631  0.369090   \n",
       "2    0.297014  0.288137  0.302547  0.330311  0.342483  0.289035  0.271342   \n",
       "3    0.276489  0.310451  0.382017  0.485955  0.568265  0.505633  0.450684   \n",
       "4    0.381110  0.376050  0.407861  0.460601  0.481486  0.389891  0.325563   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "465  0.160885  0.174962  0.209262  0.245508  0.258957  0.223890  0.225651   \n",
       "466  0.492045  0.491282  0.453160  0.393129  0.308887  0.198685  0.149238   \n",
       "467  0.171276  0.188868  0.235896  0.324066  0.461164  0.508763  0.510587   \n",
       "468  0.151263  0.158412  0.186152  0.202066  0.179934  0.118501  0.085939   \n",
       "469  0.309872  0.324136  0.363829  0.413066  0.431430  0.352381  0.300207   \n",
       "\n",
       "     volt_800  volt_801  state  \n",
       "0    0.274347  0.187275      0  \n",
       "1    0.410403  0.323149      0  \n",
       "2    0.278585  0.217022      0  \n",
       "3    0.407461  0.272598      0  \n",
       "4    0.281171  0.180556      0  \n",
       "..        ...       ...    ...  \n",
       "465  0.280271  0.285810      2  \n",
       "466  0.125311  0.074611      2  \n",
       "467  0.458503  0.272390      2  \n",
       "468  0.069742  0.040592      2  \n",
       "469  0.265476  0.173179      2  \n",
       "\n",
       "[295 rows x 1201 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>current_202</th>\n",
       "      <th>current_203</th>\n",
       "      <th>current_204</th>\n",
       "      <th>current_205</th>\n",
       "      <th>current_206</th>\n",
       "      <th>current_207</th>\n",
       "      <th>current_208</th>\n",
       "      <th>current_209</th>\n",
       "      <th>current_210</th>\n",
       "      <th>current_211</th>\n",
       "      <th>...</th>\n",
       "      <th>volt_793</th>\n",
       "      <th>volt_794</th>\n",
       "      <th>volt_795</th>\n",
       "      <th>volt_796</th>\n",
       "      <th>volt_797</th>\n",
       "      <th>volt_798</th>\n",
       "      <th>volt_799</th>\n",
       "      <th>volt_800</th>\n",
       "      <th>volt_801</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.691919</td>\n",
       "      <td>0.691857</td>\n",
       "      <td>0.691331</td>\n",
       "      <td>0.690484</td>\n",
       "      <td>0.689532</td>\n",
       "      <td>0.688726</td>\n",
       "      <td>0.688130</td>\n",
       "      <td>0.687714</td>\n",
       "      <td>0.687552</td>\n",
       "      <td>0.687673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314036</td>\n",
       "      <td>0.318342</td>\n",
       "      <td>0.356268</td>\n",
       "      <td>0.401804</td>\n",
       "      <td>0.407339</td>\n",
       "      <td>0.316755</td>\n",
       "      <td>0.256458</td>\n",
       "      <td>0.221252</td>\n",
       "      <td>0.148448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.688282</td>\n",
       "      <td>0.687451</td>\n",
       "      <td>0.686783</td>\n",
       "      <td>0.686297</td>\n",
       "      <td>0.686016</td>\n",
       "      <td>0.686165</td>\n",
       "      <td>0.687262</td>\n",
       "      <td>0.689330</td>\n",
       "      <td>0.691714</td>\n",
       "      <td>0.693787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277765</td>\n",
       "      <td>0.266636</td>\n",
       "      <td>0.272530</td>\n",
       "      <td>0.298734</td>\n",
       "      <td>0.329949</td>\n",
       "      <td>0.307095</td>\n",
       "      <td>0.308849</td>\n",
       "      <td>0.322219</td>\n",
       "      <td>0.246138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.703717</td>\n",
       "      <td>0.703216</td>\n",
       "      <td>0.703154</td>\n",
       "      <td>0.703356</td>\n",
       "      <td>0.703613</td>\n",
       "      <td>0.703891</td>\n",
       "      <td>0.704373</td>\n",
       "      <td>0.705145</td>\n",
       "      <td>0.705897</td>\n",
       "      <td>0.706303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232290</td>\n",
       "      <td>0.283721</td>\n",
       "      <td>0.357671</td>\n",
       "      <td>0.427205</td>\n",
       "      <td>0.455655</td>\n",
       "      <td>0.389299</td>\n",
       "      <td>0.366152</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.287012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.702957</td>\n",
       "      <td>0.704118</td>\n",
       "      <td>0.705578</td>\n",
       "      <td>0.706766</td>\n",
       "      <td>0.707387</td>\n",
       "      <td>0.707607</td>\n",
       "      <td>0.707736</td>\n",
       "      <td>0.707861</td>\n",
       "      <td>0.707882</td>\n",
       "      <td>0.707653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347294</td>\n",
       "      <td>0.338249</td>\n",
       "      <td>0.361233</td>\n",
       "      <td>0.389561</td>\n",
       "      <td>0.377679</td>\n",
       "      <td>0.282076</td>\n",
       "      <td>0.228494</td>\n",
       "      <td>0.216340</td>\n",
       "      <td>0.171813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>0.710469</td>\n",
       "      <td>0.708822</td>\n",
       "      <td>0.707448</td>\n",
       "      <td>0.706760</td>\n",
       "      <td>0.706676</td>\n",
       "      <td>0.706796</td>\n",
       "      <td>0.706901</td>\n",
       "      <td>0.706971</td>\n",
       "      <td>0.707142</td>\n",
       "      <td>0.707647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310868</td>\n",
       "      <td>0.323062</td>\n",
       "      <td>0.364767</td>\n",
       "      <td>0.443387</td>\n",
       "      <td>0.525665</td>\n",
       "      <td>0.484873</td>\n",
       "      <td>0.442928</td>\n",
       "      <td>0.400778</td>\n",
       "      <td>0.262214</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>0.654703</td>\n",
       "      <td>0.654392</td>\n",
       "      <td>0.654158</td>\n",
       "      <td>0.654086</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.655265</td>\n",
       "      <td>0.656777</td>\n",
       "      <td>0.658473</td>\n",
       "      <td>0.659922</td>\n",
       "      <td>0.661014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067686</td>\n",
       "      <td>0.062863</td>\n",
       "      <td>0.078773</td>\n",
       "      <td>0.102914</td>\n",
       "      <td>0.117693</td>\n",
       "      <td>0.097859</td>\n",
       "      <td>0.085690</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>0.061386</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.691066</td>\n",
       "      <td>0.693366</td>\n",
       "      <td>0.695296</td>\n",
       "      <td>0.696822</td>\n",
       "      <td>0.698040</td>\n",
       "      <td>0.699004</td>\n",
       "      <td>0.699738</td>\n",
       "      <td>0.700179</td>\n",
       "      <td>0.700178</td>\n",
       "      <td>0.699792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250543</td>\n",
       "      <td>0.279990</td>\n",
       "      <td>0.333953</td>\n",
       "      <td>0.393716</td>\n",
       "      <td>0.419058</td>\n",
       "      <td>0.355549</td>\n",
       "      <td>0.327488</td>\n",
       "      <td>0.312347</td>\n",
       "      <td>0.211438</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.716938</td>\n",
       "      <td>0.715976</td>\n",
       "      <td>0.714911</td>\n",
       "      <td>0.713888</td>\n",
       "      <td>0.712956</td>\n",
       "      <td>0.711976</td>\n",
       "      <td>0.710904</td>\n",
       "      <td>0.709947</td>\n",
       "      <td>0.709364</td>\n",
       "      <td>0.709295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262787</td>\n",
       "      <td>0.261464</td>\n",
       "      <td>0.268023</td>\n",
       "      <td>0.270470</td>\n",
       "      <td>0.250558</td>\n",
       "      <td>0.187487</td>\n",
       "      <td>0.160850</td>\n",
       "      <td>0.156544</td>\n",
       "      <td>0.107699</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>0.671696</td>\n",
       "      <td>0.671971</td>\n",
       "      <td>0.672160</td>\n",
       "      <td>0.672397</td>\n",
       "      <td>0.672658</td>\n",
       "      <td>0.672790</td>\n",
       "      <td>0.672632</td>\n",
       "      <td>0.672193</td>\n",
       "      <td>0.671623</td>\n",
       "      <td>0.670950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139477</td>\n",
       "      <td>0.127960</td>\n",
       "      <td>0.122262</td>\n",
       "      <td>0.118875</td>\n",
       "      <td>0.110584</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.073359</td>\n",
       "      <td>0.071337</td>\n",
       "      <td>0.047548</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.721452</td>\n",
       "      <td>0.720337</td>\n",
       "      <td>0.719178</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.716881</td>\n",
       "      <td>0.715988</td>\n",
       "      <td>0.715285</td>\n",
       "      <td>0.714784</td>\n",
       "      <td>0.714635</td>\n",
       "      <td>0.714874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208111</td>\n",
       "      <td>0.335710</td>\n",
       "      <td>0.476410</td>\n",
       "      <td>0.531009</td>\n",
       "      <td>0.470152</td>\n",
       "      <td>0.308952</td>\n",
       "      <td>0.213821</td>\n",
       "      <td>0.165598</td>\n",
       "      <td>0.098637</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows Ã— 1201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     current_202  current_203  current_204  current_205  current_206  \\\n",
       "125     0.691919     0.691857     0.691331     0.690484     0.689532   \n",
       "126     0.688282     0.687451     0.686783     0.686297     0.686016   \n",
       "127     0.703717     0.703216     0.703154     0.703356     0.703613   \n",
       "128     0.702957     0.704118     0.705578     0.706766     0.707387   \n",
       "129     0.710469     0.708822     0.707448     0.706760     0.706676   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "474     0.654703     0.654392     0.654158     0.654086     0.654370   \n",
       "475     0.691066     0.693366     0.695296     0.696822     0.698040   \n",
       "476     0.716938     0.715976     0.714911     0.713888     0.712956   \n",
       "477     0.671696     0.671971     0.672160     0.672397     0.672658   \n",
       "478     0.721452     0.720337     0.719178     0.717978     0.716881   \n",
       "\n",
       "     current_207  current_208  current_209  current_210  current_211  ...  \\\n",
       "125     0.688726     0.688130     0.687714     0.687552     0.687673  ...   \n",
       "126     0.686165     0.687262     0.689330     0.691714     0.693787  ...   \n",
       "127     0.703891     0.704373     0.705145     0.705897     0.706303  ...   \n",
       "128     0.707607     0.707736     0.707861     0.707882     0.707653  ...   \n",
       "129     0.706796     0.706901     0.706971     0.707142     0.707647  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "474     0.655265     0.656777     0.658473     0.659922     0.661014  ...   \n",
       "475     0.699004     0.699738     0.700179     0.700178     0.699792  ...   \n",
       "476     0.711976     0.710904     0.709947     0.709364     0.709295  ...   \n",
       "477     0.672790     0.672632     0.672193     0.671623     0.670950  ...   \n",
       "478     0.715988     0.715285     0.714784     0.714635     0.714874  ...   \n",
       "\n",
       "     volt_793  volt_794  volt_795  volt_796  volt_797  volt_798  volt_799  \\\n",
       "125  0.314036  0.318342  0.356268  0.401804  0.407339  0.316755  0.256458   \n",
       "126  0.277765  0.266636  0.272530  0.298734  0.329949  0.307095  0.308849   \n",
       "127  0.232290  0.283721  0.357671  0.427205  0.455655  0.389299  0.366152   \n",
       "128  0.347294  0.338249  0.361233  0.389561  0.377679  0.282076  0.228494   \n",
       "129  0.310868  0.323062  0.364767  0.443387  0.525665  0.484873  0.442928   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "474  0.067686  0.062863  0.078773  0.102914  0.117693  0.097859  0.085690   \n",
       "475  0.250543  0.279990  0.333953  0.393716  0.419058  0.355549  0.327488   \n",
       "476  0.262787  0.261464  0.268023  0.270470  0.250558  0.187487  0.160850   \n",
       "477  0.139477  0.127960  0.122262  0.118875  0.110584  0.083422  0.073359   \n",
       "478  0.208111  0.335710  0.476410  0.531009  0.470152  0.308952  0.213821   \n",
       "\n",
       "     volt_800  volt_801  state  \n",
       "125  0.221252  0.148448      0  \n",
       "126  0.322219  0.246138      0  \n",
       "127  0.376777  0.287012      0  \n",
       "128  0.216340  0.171813      0  \n",
       "129  0.400778  0.262214      0  \n",
       "..        ...       ...    ...  \n",
       "474  0.079646  0.061386      2  \n",
       "475  0.312347  0.211438      2  \n",
       "476  0.156544  0.107699      2  \n",
       "477  0.071337  0.047548      2  \n",
       "478  0.165598  0.098637      2  \n",
       "\n",
       "[184 rows x 1201 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = MLPClassifier(\n",
    "    hidden_layer_sizes=(150, 300),\n",
    "    max_iter=300,\n",
    "    tol=0.0000001,\n",
    "    learning_rate_init=0.1,\n",
    "    solver='sgd',\n",
    "    activation='logistic',\n",
    "    learning_rate='constant',\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.32906126\n",
      "Iteration 2, loss = 2.43659483\n",
      "Iteration 3, loss = 3.36018187\n",
      "Iteration 4, loss = 2.80039981\n",
      "Iteration 5, loss = 1.29402355\n",
      "Iteration 6, loss = 1.29894753\n",
      "Iteration 7, loss = 1.08205963\n",
      "Iteration 8, loss = 1.09541714\n",
      "Iteration 9, loss = 1.07861916\n",
      "Iteration 10, loss = 1.06136794\n",
      "Iteration 11, loss = 1.05504635\n",
      "Iteration 12, loss = 1.05016758\n",
      "Iteration 13, loss = 1.04358251\n",
      "Iteration 14, loss = 1.03372037\n",
      "Iteration 15, loss = 1.02080576\n",
      "Iteration 16, loss = 0.99710011\n",
      "Iteration 17, loss = 0.96793859\n",
      "Iteration 18, loss = 0.93174452\n",
      "Iteration 19, loss = 0.88120995\n",
      "Iteration 20, loss = 0.81290431\n",
      "Iteration 21, loss = 0.73294781\n",
      "Iteration 22, loss = 0.65458405\n",
      "Iteration 23, loss = 0.58293878\n",
      "Iteration 24, loss = 0.53230920\n",
      "Iteration 25, loss = 0.49756581\n",
      "Iteration 26, loss = 0.46809981\n",
      "Iteration 27, loss = 0.48855175\n",
      "Iteration 28, loss = 0.51840494\n",
      "Iteration 29, loss = 0.40497731\n",
      "Iteration 30, loss = 0.39644902\n",
      "Iteration 31, loss = 0.43531495\n",
      "Iteration 32, loss = 0.36772749\n",
      "Iteration 33, loss = 0.52030166\n",
      "Iteration 34, loss = 0.57549761\n",
      "Iteration 35, loss = 0.64307482\n",
      "Iteration 36, loss = 0.24829193\n",
      "Iteration 37, loss = 0.29112476\n",
      "Iteration 38, loss = 0.48066728\n",
      "Iteration 39, loss = 0.40316089\n",
      "Iteration 40, loss = 0.65289906\n",
      "Iteration 41, loss = 0.13929309\n",
      "Iteration 42, loss = 0.11928533\n",
      "Iteration 43, loss = 0.10258486\n",
      "Iteration 44, loss = 0.08820054\n",
      "Iteration 45, loss = 0.07615501\n",
      "Iteration 46, loss = 0.07698993\n",
      "Iteration 47, loss = 0.06669010\n",
      "Iteration 48, loss = 0.05058955\n",
      "Iteration 49, loss = 0.04512742\n",
      "Iteration 50, loss = 0.04107480\n",
      "Iteration 51, loss = 0.03741715\n",
      "Iteration 52, loss = 0.03290119\n",
      "Iteration 53, loss = 0.03014129\n",
      "Iteration 54, loss = 0.02761045\n",
      "Iteration 55, loss = 0.02572607\n",
      "Iteration 56, loss = 0.02391417\n",
      "Iteration 57, loss = 0.02182839\n",
      "Iteration 58, loss = 0.02030898\n",
      "Iteration 59, loss = 0.01937282\n",
      "Iteration 60, loss = 0.01807456\n",
      "Iteration 61, loss = 0.01765667\n",
      "Iteration 62, loss = 0.01657465\n",
      "Iteration 63, loss = 0.01521065\n",
      "Iteration 64, loss = 0.01446891\n",
      "Iteration 65, loss = 0.01392036\n",
      "Iteration 66, loss = 0.01327379\n",
      "Iteration 67, loss = 0.01266621\n",
      "Iteration 68, loss = 0.01237435\n",
      "Iteration 69, loss = 0.01189857\n",
      "Iteration 70, loss = 0.01124037\n",
      "Iteration 71, loss = 0.01074967\n",
      "Iteration 72, loss = 0.01049298\n",
      "Iteration 73, loss = 0.01002546\n",
      "Iteration 74, loss = 0.00974388\n",
      "Iteration 75, loss = 0.00948408\n",
      "Iteration 76, loss = 0.00932753\n",
      "Iteration 77, loss = 0.00879683\n",
      "Iteration 78, loss = 0.00855883\n",
      "Iteration 79, loss = 0.00832547\n",
      "Iteration 80, loss = 0.00803738\n",
      "Iteration 81, loss = 0.00781948\n",
      "Iteration 82, loss = 0.00760799\n",
      "Iteration 83, loss = 0.00743560\n",
      "Iteration 84, loss = 0.00714843\n",
      "Iteration 85, loss = 0.00697941\n",
      "Iteration 86, loss = 0.00683922\n",
      "Iteration 87, loss = 0.00666973\n",
      "Iteration 88, loss = 0.00652616\n",
      "Iteration 89, loss = 0.00641090\n",
      "Iteration 90, loss = 0.00624540\n",
      "Iteration 91, loss = 0.00604925\n",
      "Iteration 92, loss = 0.00589312\n",
      "Iteration 93, loss = 0.00592267\n",
      "Iteration 94, loss = 0.00561502\n",
      "Iteration 95, loss = 0.00552421\n",
      "Iteration 96, loss = 0.00541132\n",
      "Iteration 97, loss = 0.00525928\n",
      "Iteration 98, loss = 0.00518190\n",
      "Iteration 99, loss = 0.00503630\n",
      "Iteration 100, loss = 0.00494878\n",
      "Iteration 101, loss = 0.00485043\n",
      "Iteration 102, loss = 0.00475406\n",
      "Iteration 103, loss = 0.00474432\n",
      "Iteration 104, loss = 0.00457400\n",
      "Iteration 105, loss = 0.00454167\n",
      "Iteration 106, loss = 0.00442511\n",
      "Iteration 107, loss = 0.00437079\n",
      "Iteration 108, loss = 0.00426679\n",
      "Iteration 109, loss = 0.00420068\n",
      "Iteration 110, loss = 0.00413412\n",
      "Iteration 111, loss = 0.00407031\n",
      "Iteration 112, loss = 0.00399173\n",
      "Iteration 113, loss = 0.00392525\n",
      "Iteration 114, loss = 0.00386645\n",
      "Iteration 115, loss = 0.00380520\n",
      "Iteration 116, loss = 0.00374963\n",
      "Iteration 117, loss = 0.00371646\n",
      "Iteration 118, loss = 0.00367891\n",
      "Iteration 119, loss = 0.00358577\n",
      "Iteration 120, loss = 0.00354272\n",
      "Iteration 121, loss = 0.00348298\n",
      "Iteration 122, loss = 0.00344668\n",
      "Iteration 123, loss = 0.00339235\n",
      "Iteration 124, loss = 0.00333427\n",
      "Iteration 125, loss = 0.00328825\n",
      "Iteration 126, loss = 0.00324651\n",
      "Iteration 127, loss = 0.00319771\n",
      "Iteration 128, loss = 0.00315829\n",
      "Iteration 129, loss = 0.00313759\n",
      "Iteration 130, loss = 0.00307495\n",
      "Iteration 131, loss = 0.00305673\n",
      "Iteration 132, loss = 0.00300633\n",
      "Iteration 133, loss = 0.00297059\n",
      "Iteration 134, loss = 0.00293652\n",
      "Iteration 135, loss = 0.00289440\n",
      "Iteration 136, loss = 0.00286125\n",
      "Iteration 137, loss = 0.00287094\n",
      "Iteration 138, loss = 0.00278904\n",
      "Iteration 139, loss = 0.00277505\n",
      "Iteration 140, loss = 0.00276532\n",
      "Iteration 141, loss = 0.00273008\n",
      "Iteration 142, loss = 0.00268460\n",
      "Iteration 143, loss = 0.00263960\n",
      "Iteration 144, loss = 0.00262215\n",
      "Iteration 145, loss = 0.00258806\n",
      "Iteration 146, loss = 0.00257223\n",
      "Iteration 147, loss = 0.00253685\n",
      "Iteration 148, loss = 0.00250845\n",
      "Iteration 149, loss = 0.00248173\n",
      "Iteration 150, loss = 0.00245359\n",
      "Iteration 151, loss = 0.00242777\n",
      "Iteration 152, loss = 0.00240634\n",
      "Iteration 153, loss = 0.00240749\n",
      "Iteration 154, loss = 0.00236613\n",
      "Iteration 155, loss = 0.00233834\n",
      "Iteration 156, loss = 0.00231211\n",
      "Iteration 157, loss = 0.00229256\n",
      "Iteration 158, loss = 0.00227569\n",
      "Iteration 159, loss = 0.00224965\n",
      "Iteration 160, loss = 0.00224788\n",
      "Iteration 161, loss = 0.00220904\n",
      "Iteration 162, loss = 0.00218598\n",
      "Iteration 163, loss = 0.00217045\n",
      "Iteration 164, loss = 0.00215677\n",
      "Iteration 165, loss = 0.00214111\n",
      "Iteration 166, loss = 0.00211563\n",
      "Iteration 167, loss = 0.00209256\n",
      "Iteration 168, loss = 0.00207572\n",
      "Iteration 169, loss = 0.00205778\n",
      "Iteration 170, loss = 0.00204241\n",
      "Iteration 171, loss = 0.00202983\n",
      "Iteration 172, loss = 0.00200711\n",
      "Iteration 173, loss = 0.00199391\n",
      "Iteration 174, loss = 0.00197447\n",
      "Iteration 175, loss = 0.00195546\n",
      "Iteration 176, loss = 0.00194078\n",
      "Iteration 177, loss = 0.00192597\n",
      "Iteration 178, loss = 0.00191917\n",
      "Iteration 179, loss = 0.00189841\n",
      "Iteration 180, loss = 0.00188902\n",
      "Iteration 181, loss = 0.00187555\n",
      "Iteration 182, loss = 0.00185836\n",
      "Iteration 183, loss = 0.00184273\n",
      "Iteration 184, loss = 0.00182464\n",
      "Iteration 185, loss = 0.00181055\n",
      "Iteration 186, loss = 0.00180268\n",
      "Iteration 187, loss = 0.00179334\n",
      "Iteration 188, loss = 0.00177408\n",
      "Iteration 189, loss = 0.00176532\n",
      "Iteration 190, loss = 0.00176125\n",
      "Iteration 191, loss = 0.00173572\n",
      "Iteration 192, loss = 0.00172374\n",
      "Iteration 193, loss = 0.00170916\n",
      "Iteration 194, loss = 0.00170443\n",
      "Iteration 195, loss = 0.00169134\n",
      "Iteration 196, loss = 0.00167311\n",
      "Iteration 197, loss = 0.00166085\n",
      "Iteration 198, loss = 0.00165290\n",
      "Iteration 199, loss = 0.00164082\n",
      "Iteration 200, loss = 0.00162891\n",
      "Iteration 201, loss = 0.00161679\n",
      "Iteration 202, loss = 0.00160704\n",
      "Iteration 203, loss = 0.00159650\n",
      "Iteration 204, loss = 0.00158586\n",
      "Iteration 205, loss = 0.00157617\n",
      "Iteration 206, loss = 0.00156417\n",
      "Iteration 207, loss = 0.00155794\n",
      "Iteration 208, loss = 0.00154767\n",
      "Iteration 209, loss = 0.00153684\n",
      "Iteration 210, loss = 0.00154065\n",
      "Iteration 211, loss = 0.00151597\n",
      "Iteration 212, loss = 0.00150370\n",
      "Iteration 213, loss = 0.00150126\n",
      "Iteration 214, loss = 0.00149192\n",
      "Iteration 215, loss = 0.00148184\n",
      "Iteration 216, loss = 0.00147142\n",
      "Iteration 217, loss = 0.00146139\n",
      "Iteration 218, loss = 0.00144940\n",
      "Iteration 219, loss = 0.00144022\n",
      "Iteration 220, loss = 0.00143258\n",
      "Iteration 221, loss = 0.00142925\n",
      "Iteration 222, loss = 0.00141611\n",
      "Iteration 223, loss = 0.00140868\n",
      "Iteration 224, loss = 0.00139882\n",
      "Iteration 225, loss = 0.00139393\n",
      "Iteration 226, loss = 0.00138414\n",
      "Iteration 227, loss = 0.00138032\n",
      "Iteration 228, loss = 0.00136798\n",
      "Iteration 229, loss = 0.00135818\n",
      "Iteration 230, loss = 0.00135070\n",
      "Iteration 231, loss = 0.00134285\n",
      "Iteration 232, loss = 0.00133634\n",
      "Iteration 233, loss = 0.00132817\n",
      "Iteration 234, loss = 0.00132093\n",
      "Iteration 235, loss = 0.00131408\n",
      "Iteration 236, loss = 0.00130629\n",
      "Iteration 237, loss = 0.00129908\n",
      "Iteration 238, loss = 0.00129277\n",
      "Iteration 239, loss = 0.00128799\n",
      "Iteration 240, loss = 0.00127928\n",
      "Iteration 241, loss = 0.00127249\n",
      "Iteration 242, loss = 0.00126519\n",
      "Iteration 243, loss = 0.00125911\n",
      "Iteration 244, loss = 0.00125353\n",
      "Iteration 245, loss = 0.00124876\n",
      "Iteration 246, loss = 0.00124440\n",
      "Iteration 247, loss = 0.00123449\n",
      "Iteration 248, loss = 0.00122710\n",
      "Iteration 249, loss = 0.00122106\n",
      "Iteration 250, loss = 0.00121607\n",
      "Iteration 251, loss = 0.00121074\n",
      "Iteration 252, loss = 0.00120448\n",
      "Iteration 253, loss = 0.00119922\n",
      "Iteration 254, loss = 0.00119168\n",
      "Iteration 255, loss = 0.00118682\n",
      "Iteration 256, loss = 0.00117970\n",
      "Iteration 257, loss = 0.00117375\n",
      "Iteration 258, loss = 0.00116825\n",
      "Iteration 259, loss = 0.00116329\n",
      "Iteration 260, loss = 0.00115690\n",
      "Iteration 261, loss = 0.00115149\n",
      "Iteration 262, loss = 0.00114696\n",
      "Iteration 263, loss = 0.00114328\n",
      "Iteration 264, loss = 0.00113708\n",
      "Iteration 265, loss = 0.00113229\n",
      "Iteration 266, loss = 0.00112927\n",
      "Iteration 267, loss = 0.00112360\n",
      "Iteration 268, loss = 0.00111460\n",
      "Iteration 269, loss = 0.00111011\n",
      "Iteration 270, loss = 0.00110555\n",
      "Iteration 271, loss = 0.00110052\n",
      "Iteration 272, loss = 0.00109468\n",
      "Iteration 273, loss = 0.00109112\n",
      "Iteration 274, loss = 0.00108574\n",
      "Iteration 275, loss = 0.00108061\n",
      "Iteration 276, loss = 0.00107610\n",
      "Iteration 277, loss = 0.00107151\n",
      "Iteration 278, loss = 0.00106644\n",
      "Iteration 279, loss = 0.00106246\n",
      "Iteration 280, loss = 0.00105703\n",
      "Iteration 281, loss = 0.00105273\n",
      "Iteration 282, loss = 0.00104854\n",
      "Iteration 283, loss = 0.00104408\n",
      "Iteration 284, loss = 0.00104033\n",
      "Iteration 285, loss = 0.00103574\n",
      "Iteration 286, loss = 0.00103199\n",
      "Iteration 287, loss = 0.00102651\n",
      "Iteration 288, loss = 0.00102137\n",
      "Iteration 289, loss = 0.00101695\n",
      "Iteration 290, loss = 0.00101369\n",
      "Iteration 291, loss = 0.00101001\n",
      "Iteration 292, loss = 0.00100646\n",
      "Iteration 293, loss = 0.00100104\n",
      "Iteration 294, loss = 0.00099702\n",
      "Iteration 295, loss = 0.00099229\n",
      "Iteration 296, loss = 0.00098892\n",
      "Iteration 297, loss = 0.00098416\n",
      "Iteration 298, loss = 0.00098089\n",
      "Iteration 299, loss = 0.00097704\n",
      "Iteration 300, loss = 0.00097282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Brincadeiras/facul/ia/ann-engine/venv/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(150, 300),\n",
       "              learning_rate_init=0.1, max_iter=300, solver=&#x27;sgd&#x27;, tol=1e-07,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(150, 300),\n",
       "              learning_rate_init=0.1, max_iter=300, solver=&#x27;sgd&#x27;, tol=1e-07,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='logistic', hidden_layer_sizes=(150, 300),\n",
       "              learning_rate_init=0.1, max_iter=300, solver='sgd', tol=1e-07,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\n"
     ]
    }
   ],
   "source": [
    "score = ann.score(X_test, y_test)\n",
    "\n",
    "print('%.2f' %(score*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the probability for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a random sample of state 0: [[9.996e-01 0.000e+00 4.000e-04]]\n",
      "Probability of a random sample of state 1: [[0.     0.9989 0.0011]]\n",
      "Probability of a random sample of state 2: [[9.00e-04 2.00e-04 9.99e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of a random sample of state 0:\", np.round(ann.predict_proba(X_test_0[:1]), 4) )\n",
    "print(\"Probability of a random sample of state 1:\", np.round(ann.predict_proba(X_test_1[:1]), 4) )\n",
    "print(\"Probability of a random sample of state 2:\", np.round(ann.predict_proba(X_test_2[:1]), 4) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO: CHECK OVERFITTING AND UNDERFITTING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b23d1aa7f2d5a2e3a6d1b4e4b9b224696db3497f0028adef9830a45a6035367"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
